from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import time
from datetime import datetime

# Imports de tous nos composants
from src.retrieval.embedder import DocumentEmbedder
from src.retrieval.vector_store import VectorStore
from src.retrieval.retriever import BaseRetriever
from src.grading.knowledge_stripper import KnowledgeStripper
from src.grading.relevance_grader import RelevanceGrader
from src.refinement.knowledge_refiner import KnowledgeRefiner
from src.generation.response_generator import ResponseGenerator, GeneratedResponse


@dataclass
class CRAGResult:
    """
    R√©sultat complet du pipeline CRAG.
    Contient la r√©ponse et toutes les m√©tadonn√©es du processus.
    """
    query: str
    answer: str
    confidence: float
    processing_time: float
    steps_details: Dict[str, any]
    needs_more_info: bool
    debug_info: Optional[Dict] = None


class CRAGPipeline:
    """
    Pipeline Corrective RAG complet.

    Ce pipeline orchestre tout le processus :
    1. Retrieval : Trouve les documents pertinents
    2. Stripping : D√©coupe en unit√©s de connaissance
    3. Grading : √âvalue la pertinence
    4. Refinement : Filtre et organise
    5. Generation : Produit la r√©ponse finale

    C'est le chef d'orchestre qui fait travailler tous les composants ensemble.
    """

    def __init__(self,
                 embedder: Optional[DocumentEmbedder] = None,
                 retriever: Optional[BaseRetriever] = None,
                 stripper: Optional[KnowledgeStripper] = None,
                 grader: Optional[RelevanceGrader] = None,
                 refiner: Optional[KnowledgeRefiner] = None,
                 generator: Optional[ResponseGenerator] = None,
                 verbose: bool = True):
        """
        Initialise le pipeline avec tous ses composants.
        Si un composant n'est pas fourni, il sera cr√©√© avec les param√®tres par d√©faut.
        """
        self.verbose = verbose

        # Initialiser les composants
        self._log("üöÄ Initialisation du pipeline CRAG...")

        # Embedder (partag√© entre plusieurs composants)
        self.embedder = embedder or DocumentEmbedder()

        # Retriever
        self.retriever = retriever or BaseRetriever(self.embedder)

        # Knowledge Stripper
        self.stripper = stripper or KnowledgeStripper()

        # Relevance Grader
        self.grader = grader or RelevanceGrader(
            use_llm=False,  # Par d√©faut sans LLM
            embedder=self.embedder
        )

        # Knowledge Refiner
        self.refiner = refiner or KnowledgeRefiner(
            embedder=self.embedder,
            relevance_threshold=0.5,
            min_coverage=0.6
        )

        # Response Generator
        self.generator = generator or ResponseGenerator(
            use_templates=True,
            response_style="educational"
        )

        self._log("‚úÖ Pipeline CRAG initialis√© avec succ√®s!")

    def index_documents(self,
                        documents: List[str],
                        metadata: Optional[List[Dict]] = None,
                        batch_size: int = 32):
        """
        Indexe des documents dans la base de connaissances.

        C'est la pr√©paration : on stocke les documents pour pouvoir
        les retrouver plus tard quand on aura des questions.
        """
        self._log(f"üìö Indexation de {len(documents)} documents...")
        start_time = time.time()

        self.retriever.index_documents(documents, metadata, batch_size)

        indexing_time = time.time() - start_time
        self._log(f"‚úÖ Indexation termin√©e en {indexing_time:.2f}s")

    def process_query(self,
                      query: str,
                      max_strips: int = 10,
                      debug: bool = False) -> CRAGResult:
        """
        Traite une requ√™te de bout en bout.

        C'est ici que la magie op√®re : la question entre,
        une r√©ponse structur√©e et v√©rifi√©e sort.
        """
        self._log(f"\n{'=' * 60}")
        self._log(f"üîç NOUVELLE REQU√äTE : {query}")
        self._log(f"{'=' * 60}")

        start_time = time.time()
        steps_details = {}

        try:
            # √âtape 1 : Retrieval
            retrieved_docs = self._step_retrieval(query, steps_details)

            # √âtape 2 : Knowledge Stripping
            all_strips = self._step_stripping(retrieved_docs, steps_details)

            # √âtape 3 : Relevance Grading
            graded_strips = self._step_grading(query, all_strips, steps_details)

            # √âtape 4 : Knowledge Refinement
            refined_knowledge = self._step_refinement(query, graded_strips, steps_details)

            # √âtape 5 : Response Generation
            response = self._step_generation(query, refined_knowledge, steps_details)

            # Calculer le temps total
            processing_time = time.time() - start_time

            # Cr√©er le r√©sultat final
            result = CRAGResult(
                query=query,
                answer=response.answer,
                confidence=response.confidence,
                processing_time=processing_time,
                steps_details=steps_details,
                needs_more_info=refined_knowledge.needs_additional_search,
                debug_info=self._create_debug_info(
                    retrieved_docs,
                    graded_strips,
                    refined_knowledge,
                    response
                ) if debug else None
            )

            self._log_final_summary(result)

            return result

        except Exception as e:
            self._log(f"‚ùå ERREUR : {str(e)}", error=True)
            # Retourner un r√©sultat d'erreur
            return CRAGResult(
                query=query,
                answer=f"D√©sol√©, une erreur s'est produite : {str(e)}",
                confidence=0.0,
                processing_time=time.time() - start_time,
                steps_details=steps_details,
                needs_more_info=True
            )

    def _step_retrieval(self, query: str, steps_details: Dict) -> List[Dict]:
        """
        √âtape 1 : R√©cup√©ration des documents pertinents.
        """
        self._log("\nüìö √âTAPE 1 : RETRIEVAL")
        start = time.time()

        # R√©cup√©rer les documents
        retrieved_docs = self.retriever.retrieve(query, k=5)

        # Enregistrer les d√©tails
        steps_details["retrieval"] = {
            "duration": time.time() - start,
            "docs_retrieved": len(retrieved_docs),
            "top_scores": [doc["score"] for doc in retrieved_docs[:3]]
        }

        self._log(f"   ‚Üí {len(retrieved_docs)} documents r√©cup√©r√©s")
        for i, doc in enumerate(retrieved_docs[:3]):
            self._log(f"   ‚Üí Doc {i + 1} (score: {doc['score']:.3f}): {doc['text'][:50]}...")

        return retrieved_docs

    def _step_stripping(self, retrieved_docs: List[Dict], steps_details: Dict) -> List:
        """
        √âtape 2 : D√©coupage en knowledge strips.
        """
        self._log("\n‚úÇÔ∏è  √âTAPE 2 : KNOWLEDGE STRIPPING")
        start = time.time()

        all_strips = []
        strip_counts = []

        for doc in retrieved_docs:
            strips = self.stripper.strip_document(
                doc['text'],
                doc['index'],
                granularity="adaptive"
            )
            all_strips.extend(strips)
            strip_counts.append(len(strips))

        steps_details["stripping"] = {
            "duration": time.time() - start,
            "total_strips": len(all_strips),
            "strips_per_doc": strip_counts
        }

        self._log(f"   ‚Üí {len(all_strips)} strips extraits au total")
        self._log(f"   ‚Üí Distribution : {strip_counts}")

        return all_strips

    def _step_grading(self, query: str, all_strips: List, steps_details: Dict) -> List:
        """
        √âtape 3 : √âvaluation de la pertinence.
        """
        self._log("\n‚≠ê √âTAPE 3 : RELEVANCE GRADING")
        start = time.time()

        # Grader tous les strips
        graded_strips = self.grader.grade_strips(query, all_strips)

        # Analyser la distribution des scores
        score_distribution = {
            "HIGHLY_RELEVANT": sum(1 for s in graded_strips if s.relevance_score >= 0.8),
            "RELEVANT": sum(1 for s in graded_strips if 0.65 <= s.relevance_score < 0.8),
            "SOMEWHAT_RELEVANT": sum(1 for s in graded_strips if 0.5 <= s.relevance_score < 0.65),
            "LOW_RELEVANCE": sum(1 for s in graded_strips if s.relevance_score < 0.5)
        }

        steps_details["grading"] = {
            "duration": time.time() - start,
            "total_graded": len(graded_strips),
            "score_distribution": score_distribution,
            "top_scores": [s.relevance_score for s in graded_strips[:5]]
        }

        self._log(f"   ‚Üí Distribution des scores : {score_distribution}")
        self._log(f"   ‚Üí Top 3 strips :")
        for i, strip in enumerate(graded_strips[:3]):
            self._log(f"      {i + 1}. [{strip.relevance_category.name}] {strip.strip.content[:60]}...")

        return graded_strips

    def _step_refinement(self, query: str, graded_strips: List, steps_details: Dict):
        """
        √âtape 4 : Raffinement et organisation.
        """
        self._log("\nüîß √âTAPE 4 : KNOWLEDGE REFINEMENT")
        start = time.time()

        # Raffiner les connaissances
        refined_knowledge = self.refiner.refine_knowledge(graded_strips, query)

        steps_details["refinement"] = {
            "duration": time.time() - start,
            "strips_before": len(graded_strips),
            "strips_after": len(refined_knowledge.strips),
            "coverage_score": refined_knowledge.coverage_score,
            "needs_more_info": refined_knowledge.needs_additional_search
        }

        # Log d√©j√† affich√© par le refiner

        return refined_knowledge

    def _step_generation(self, query: str, refined_knowledge, steps_details: Dict):
        """
        √âtape 5 : G√©n√©ration de la r√©ponse.
        """
        self._log("\n‚úçÔ∏è  √âTAPE 5 : RESPONSE GENERATION")
        start = time.time()

        # D√©terminer le type de question
        question_type = self._detect_question_type(query)

        # G√©n√©rer la r√©ponse
        response = self.generator.generate_response(
            query=query,
            refined_knowledge=refined_knowledge,
            question_type=question_type
        )

        steps_details["generation"] = {
            "duration": time.time() - start,
            "method": response.generation_method,
            "response_length": len(response.answer),
            "confidence": response.confidence
        }

        self._log(f"   ‚Üí M√©thode : {response.generation_method}")
        self._log(f"   ‚Üí Longueur : {len(response.answer)} caract√®res")
        self._log(f"   ‚Üí Confiance : {response.confidence:.2%}")

        return response

    def _detect_question_type(self, query: str) -> str:
        """
        D√©tecte le type de question pour adapter la g√©n√©ration.
        """
        query_lower = query.lower()

        if any(word in query_lower for word in ["comment", "fonctionne", "marche"]):
            return "comment_fonctionne"
        elif any(word in query_lower for word in ["qu'est-ce", "quoi", "d√©finition"]):
            return "qu_est_ce_que"
        elif "pourquoi" in query_lower:
            return "pourquoi"
        else:
            return "general"

    def _create_debug_info(self, retrieved_docs, graded_strips, refined_knowledge, response):
        """
        Cr√©e des informations de d√©bogage d√©taill√©es.
        """
        return {
            "retrieved_docs": [
                {
                    "score": doc["score"],
                    "text_preview": doc["text"][:100] + "...",
                    "metadata": doc.get("metadata", {})
                }
                for doc in retrieved_docs
            ],
            "top_graded_strips": [
                {
                    "score": strip.relevance_score,
                    "category": strip.relevance_category.name,
                    "type": strip.strip.strip_type,
                    "content": strip.strip.content[:100] + "..."
                }
                for strip in graded_strips[:5]
            ],
            "refinement_summary": refined_knowledge.summary,
            "generation_details": response.metadata
        }

    def _log_final_summary(self, result: CRAGResult):
        """
        Affiche un r√©sum√© final du traitement.
        """
        self._log(f"\n{'=' * 60}")
        self._log("üìä R√âSUM√â DU TRAITEMENT")
        self._log(f"{'=' * 60}")
        self._log(f"‚è±Ô∏è  Temps total : {result.processing_time:.2f}s")
        self._log(f"üéØ Confiance : {result.confidence:.2%}")

        if result.needs_more_info:
            self._log("‚ö†Ô∏è  Recherche suppl√©mentaire recommand√©e")
        else:
            self._log("‚úÖ Information suffisante pour r√©pondre")

        # D√©tail par √©tape
        self._log("\nüìà Temps par √©tape :")
        for step, details in result.steps_details.items():
            if "duration" in details:
                self._log(f"   - {step.capitalize()} : {details['duration']:.3f}s")

    def _log(self, message: str, error: bool = False):
        """
        Affiche un message de log si verbose est activ√©.
        """
        if self.verbose:
            timestamp = datetime.now().strftime("%H:%M:%S")
            prefix = "‚ùå" if error else "‚ÑπÔ∏è"
            print(f"[{timestamp}] {message}")