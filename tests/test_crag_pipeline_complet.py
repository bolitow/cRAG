# Test complet du pipeline CRAG
import sys

sys.path.append('../src')

from pipeline.crag_pipeline import CRAGPipeline

# Cr√©er le pipeline complet
print("üöÄ Initialisation du pipeline CRAG complet...")
pipeline = CRAGPipeline(verbose=True)

# Documents de test - Une base de connaissances sur les transformers
documents = [
    """Les transformers sont une architecture de r√©seau de neurones r√©volutionnaire introduite en 2017 
    dans le papier "Attention is All You Need". Ils ont transform√© le domaine du traitement du langage naturel
    en permettant un traitement parall√®le efficace des s√©quences, contrairement aux RNN qui traitent
    les donn√©es s√©quentiellement.""",

    """Le m√©canisme d'attention est le c≈ìur des transformers. Il calcule des scores d'attention entre
    tous les √©l√©ments d'une s√©quence, permettant au mod√®le de capturer des d√©pendances √† long terme.
    L'attention multi-t√™tes divise la repr√©sentation en plusieurs sous-espaces, permettant au mod√®le
    d'apprendre diff√©rents types de relations simultan√©ment.""",

    """Les composants principaux d'un transformer incluent :
    - L'encodeur : transforme la s√©quence d'entr√©e en repr√©sentations
    - Le d√©codeur : g√©n√®re la s√©quence de sortie
    - Les couches d'attention multi-t√™tes
    - Les r√©seaux feed-forward
    - La normalisation par couches (Layer Norm)
    - Les connexions r√©siduelles
    - L'encodage positionnel pour capturer l'ordre des mots""",

    """BERT (Bidirectional Encoder Representations from Transformers) utilise uniquement la partie
    encodeur des transformers. Il est pr√©-entra√Æn√© sur deux t√¢ches : pr√©diction de mots masqu√©s
    et pr√©diction de la phrase suivante. Cette approche bidirectionnelle lui permet de comprendre
    le contexte complet d'un mot.""",

    """GPT (Generative Pre-trained Transformer) utilise uniquement la partie d√©codeur des transformers.
    Il est entra√Æn√© de mani√®re autoregressive, pr√©disant le mot suivant √©tant donn√© tous les mots
    pr√©c√©dents. Cette approche le rend excellent pour la g√©n√©ration de texte.""",

    """Les transformers ont r√©volutionn√© de nombreux domaines au-del√† du NLP. Vision Transformer (ViT)
    applique l'architecture aux images en les d√©coupant en patches. DALL-E utilise les transformers
    pour g√©n√©rer des images √† partir de descriptions textuelles. Les transformers sont maintenant
    utilis√©s en biologie, musique, et m√™me en chimie."""
]

# M√©tadonn√©es pour tracer l'origine
metadata = [
    {"source": "introduction_transformers.pdf", "section": "Overview"},
    {"source": "attention_mechanism.pdf", "section": "Core Concepts"},
    {"source": "transformer_architecture.pdf", "section": "Components"},
    {"source": "bert_paper.pdf", "section": "Model Description"},
    {"source": "gpt_paper.pdf", "section": "Architecture"},
    {"source": "transformers_applications.pdf", "section": "Beyond NLP"}
]

# Indexer les documents
print("\nüìö Indexation de la base de connaissances...")
pipeline.index_documents(documents, metadata)

# Tester plusieurs questions
questions = [
    "Comment fonctionnent les transformers ?",
    "Qu'est-ce que BERT ?",
    "Quelle est la diff√©rence entre BERT et GPT ?",
    "Pourquoi les transformers sont-ils r√©volutionnaires ?",
    "Quels sont les composants d'un transformer ?"
]

print("\nüéØ Test du pipeline avec diff√©rentes questions...\n")

for i, question in enumerate(questions, 1):
    print(f"\n{'#' * 80}")
    print(f"QUESTION {i}/{len(questions)}")
    print(f"{'#' * 80}")

    # Traiter la question
    result = pipeline.process_query(question, debug=True)

    # Afficher la r√©ponse
    print(f"\n{'=' * 60}")
    print("üìù R√âPONSE G√âN√âR√âE")
    print(f"{'=' * 60}")
    print(result.answer)

    # Afficher les m√©tadonn√©es
    print(f"\n{'=' * 60}")
    print("üìä M√âTADONN√âES")
    print(f"{'=' * 60}")
    print(f"Confiance : {result.confidence:.2%}")
    print(f"Temps de traitement : {result.processing_time:.2f}s")
    print(f"Besoin d'infos suppl√©mentaires : {'Oui' if result.needs_more_info else 'Non'}")

    # Si mode debug, afficher plus de d√©tails
    if result.debug_info:
        print(f"\nüîç DEBUG - Top strips utilis√©s :")
        for j, strip in enumerate(result.debug_info["top_graded_strips"][:3], 1):
            print(f"   {j}. [{strip['category']}] {strip['content']}")

    # Pause entre les questions
    if i < len(questions):
        input(f"\n>>> Appuyez sur Entr√©e pour la question suivante...")

# R√©sum√© final
print(f"\n{'=' * 80}")
print("üèÅ TEST TERMIN√â")
print(f"{'=' * 80}")
print(f"Questions trait√©es : {len(questions)}")
print("Le pipeline CRAG est op√©rationnel ! üéâ")