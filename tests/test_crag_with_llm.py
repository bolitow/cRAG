# tests/test_crag_with_llm.py
import sys

sys.path.append('../src')

from pipeline.crag_pipeline import CRAGPipeline
from generation.response_generator import ResponseGenerator

# CrÃ©er le gÃ©nÃ©rateur avec ton endpoint LLM
generator = ResponseGenerator(
    llm_endpoint="http://141.94.106.229:8085/generate",
    use_llm=True,
    use_templates=False,  # DÃ©sactiver les templates pour utiliser le LLM
    response_style="educational"
)

# CrÃ©er le pipeline avec le gÃ©nÃ©rateur LLM
print("ğŸš€ Initialisation du pipeline CRAG avec LLM...")
pipeline = CRAGPipeline(
    generator=generator,
    verbose=True
)

# Documents de test
documents = [
    """Les transformers sont une architecture de rÃ©seau de neurones rÃ©volutionnaire introduite en 2017.
    Ils utilisent le mÃ©canisme d'attention pour traiter les sÃ©quences de maniÃ¨re parallÃ¨le,
    contrairement aux RNN qui traitent les donnÃ©es sÃ©quentiellement.""",

    """Le mÃ©canisme d'attention est le cÅ“ur des transformers. Il calcule des scores d'attention
    entre tous les Ã©lÃ©ments d'une sÃ©quence. L'attention multi-tÃªtes permet au modÃ¨le
    d'apprendre diffÃ©rents types de relations simultanÃ©ment.""",

    """BERT (Bidirectional Encoder Representations from Transformers) utilise uniquement
    la partie encodeur. Il est prÃ©-entraÃ®nÃ© sur la prÃ©diction de mots masquÃ©s et comprend
    le contexte bidirectionnel complet."""
]

# Indexer les documents
pipeline.index_documents(documents)

# Tester une question
question = "Comment fonctionnent les transformers ?"
print(f"\nğŸ” Question : {question}")

# Traiter la question
result = pipeline.process_query(question)

# Afficher la rÃ©ponse
print(f"\nğŸ“ RÃ‰PONSE GÃ‰NÃ‰RÃ‰E PAR LLM :")
print("=" * 60)
print(result.answer)
print("=" * 60)
print(f"\nMÃ©thode utilisÃ©e : {result.steps_details['generation']['method']}")
print(f"Confiance : {result.confidence:.2%}")